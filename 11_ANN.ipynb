{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Labels : [0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "## Dataset\n",
    "dataset = [\n",
    "    \"သူမသည် ဖျက်သိမ်း စာကို ဖတ်နေစဉ် မျက်ရည်များ စီးကျလို့ နေသည်။\",\n",
    "    \"အခန်းသည် သူမ မရှိခြင်းကြောင့် တိတ်ဆိတ်မှု တို့ဖြင့် လွှမ်းမိုး စေခဲ့သည်။\",\n",
    "    \"သူသည် အလင်း မရှိသော နေရာတွင် တစ်ယောက်တည်း ထိုင်ပြီး ဝမ်းနည်းမှု ၏ အငွေ့အသက်ကို ခံစား နေသည်။\",\n",
    "    \"သူမ၏ နှလုံးသားသည် မပြောလိုက်သော စကားများ၏ နာကျင်မှု ကြောင့် ဝမ်းနည်း နေသည်။\",\n",
    "    \"မိုးသည် သူမ၏ စိတ်ခံစားချက်ကို စာနာပြီး ဝမ်းနည်းစွာ ကောင်းကင်မှ မျက်ရည်စက်များ အဖြစ် ရွာချပေးလို့ နေသည်။\",\n",
    "\n",
    "    \"သူ၏ အပြုံးက တစ်ခန်းလုံးကို ထွန်းလင်းစေပြီး သူမ၏ နှလုံးသားကို ပူနွေး စေခဲ့သည်။\",\n",
    "    \"သူမနှင့် တွေ့ဆုံတိုင်း ခံစားချက် တို့သည် အိမ်မက် တစ်ခု အလား အလွန်ပင် သာယာလှပလို့ နေသည်။\",\n",
    "    \"အချစ်၏ စွမ်းအား တို့သည် ချစ်သူ နှစ်ဦး ကြားရှိ အကွာအဝေးကို ဖျက်ဆီးပစ်နိုင်သည်။\",\n",
    "    \"အချစ်သည် သက်တမ်းမဲ့ တန်ခိုး တစ်ခု ဖြစ်ပြီး နှလုံးသားများကို နူးညံ့စေသည်။\",\n",
    "    \"အချစ်ကို သန့်ရှင်းစွာ ထိန်းသိမ်းနိုင်ပါက ဘဝ၏ အလှတရားကို ခံစားရမည်။\",\n",
    "\n",
    "    \"သူ၏ လက်သီးကို ကျစ်ကျစ် ဆုတ်ထားပြီး ရန်ငြိုးကို ထိန်းထားရန် ကြိုးစားနေသည်။\",\n",
    "    \"သူမသည် တံခါးကို ပိတ်ချပြီး ဆူညံစွာ အော်ဟစ်လို့ နေသည်။\",\n",
    "    \"သူ၏ မျက်လုံးများတွင် ဒေါသ စိတ် တို့ဖြင့် မီးဟုန်းဟုန်း လောင်နေသည်။\",\n",
    "    \"အငြင်းပွားမှု သည် မာနကြောင့် တိုး၍ တိုး၍ လာသည်။\",\n",
    "    \"သူမ၏ အသံသည် စိတ်ဆိုးမှု ကြောင့် တုန်လှုပ်နေပြီး စူးစိမ့် လှသည်။\"\n",
    "]\n",
    "targets = [\"Sadness\", \"Sadness\", \"Sadness\", \"Sadness\", \"Sadness\", \"Love\", \"Love\", \"Love\", \"Love\", \"Love\", \"Anger\", \"Anger\", \"Anger\", \"Anger\", \"Anger\"]\n",
    "\n",
    "## Convert the target labels to numerical values\n",
    "target_mapping = {\"Sadness\": 0, \"Love\": 1, \"Anger\": 2}\n",
    "encoded_lbls = np.array([target_mapping[target] for target in targets])\n",
    "print(f\"Encoded Labels : {encoded_lbls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sentences : [[33, 13, 81, 94, 23, 54, 99], [76, 58, 44, 7, 42, 8, 28], [35, 39, 67, 69, 51, 66, 49, 38, 97, 44, 99], [60, 49, 3, 96, 27, 70, 38, 99], [75, 60, 53, 93, 62, 45, 12, 60, 61, 99], [70, 5, 39, 44, 60, 68, 37, 28], [18, 98, 21, 63, 93, 70, 52, 57, 70, 99], [50, 3, 63, 26, 68, 43, 16, 48], [75, 33, 63, 70, 92, 76, 10], [11, 26, 65, 85, 41, 81], [70, 53, 25, 56, 17, 73, 70], [33, 79, 38, 63, 50, 99], [70, 98, 97, 20, 42, 17, 27], [86, 69, 93, 76, 76, 43], [60, 79, 49, 70, 72, 11, 97]]\n"
     ]
    }
   ],
   "source": [
    "## Apply One Hot\n",
    "vocab_size = 100\n",
    "encoded_sentences = [one_hot(i, vocab_size) for i in dataset]\n",
    "print(f\"Encoded Sentences : {encoded_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding : [[ 0  0  0  0  0  0  0  0 33 13 81 94 23 54 99]\n",
      " [ 0  0  0  0  0  0  0  0 76 58 44  7 42  8 28]\n",
      " [ 0  0  0  0 35 39 67 69 51 66 49 38 97 44 99]\n",
      " [ 0  0  0  0  0  0  0 60 49  3 96 27 70 38 99]\n",
      " [ 0  0  0  0  0 75 60 53 93 62 45 12 60 61 99]\n",
      " [ 0  0  0  0  0  0  0 70  5 39 44 60 68 37 28]\n",
      " [ 0  0  0  0  0 18 98 21 63 93 70 52 57 70 99]\n",
      " [ 0  0  0  0  0  0  0 50  3 63 26 68 43 16 48]\n",
      " [ 0  0  0  0  0  0  0  0 75 33 63 70 92 76 10]\n",
      " [ 0  0  0  0  0  0  0  0  0 11 26 65 85 41 81]\n",
      " [ 0  0  0  0  0  0  0  0 70 53 25 56 17 73 70]\n",
      " [ 0  0  0  0  0  0  0  0  0 33 79 38 63 50 99]\n",
      " [ 0  0  0  0  0  0  0  0 70 98 97 20 42 17 27]\n",
      " [ 0  0  0  0  0  0  0  0  0 86 69 93 76 76 43]\n",
      " [ 0  0  0  0  0  0  0  0 60 79 49 70 72 11 97]]\n"
     ]
    }
   ],
   "source": [
    "## Apply Padding\n",
    "max_length = 15\n",
    "pad_sentences = pad_sequences(encoded_sentences, maxlen=max_length, padding='pre')\n",
    "print(f\"Padding : {pad_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 15, 10)            1000      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 150)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 453       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,453\n",
      "Trainable params: 1,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Create ANN Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=10, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "## Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.9075 - accuracy: 1.0000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9010 - accuracy: 1.0000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8946 - accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8880 - accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8814 - accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8748 - accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8681 - accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8613 - accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8545 - accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8477 - accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8407 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8338 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8268 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8197 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8126 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8055 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7983 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7910 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7838 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7764 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7691 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7617 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7542 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7467 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7392 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7317 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7241 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7165 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7088 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7012 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6935 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6858 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6780 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6703 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6625 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6548 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6470 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6392 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6314 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6236 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6158 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6080 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6003 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5925 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5847 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5770 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5693 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5616 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5539 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5462 - accuracy: 1.0000\n",
      "Accuracy : 1.0, Loss : 0.5386006832122803\n"
     ]
    }
   ],
   "source": [
    "## Train Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(pad_sentences, encoded_lbls, epochs=50)\n",
    "\n",
    "## Model Evaluation\n",
    "loss, acc = model.evaluate(pad_sentences, encoded_lbls, verbose=0)\n",
    "print(f'Accuracy : {acc}, Loss : {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions : ['Sadness', 'Love', 'Anger']\n"
     ]
    }
   ],
   "source": [
    "## Inference Model\n",
    "test = [\n",
    "    \"သူမ၏ နှလုံးသားသည် မပြောလိုက်သော စကားများ၏ နာကျင်မှု ကြောင့် ဝမ်းနည်း နေသည်။\",\n",
    "    \"အချစ်သည် သက်တမ်းမဲ့ တန်ခိုး တစ်ခု ဖြစ်ပြီး နှလုံးသားများကို နူးညံ့စေသည်။\",\n",
    "    \"အငြင်းပွားမှု သည် မာနကြောင့် တိုး၍ တိုး၍ လာသည်။\"\n",
    "]\n",
    "\n",
    "encoded_test = [one_hot(txt, vocab_size) for txt in test]\n",
    "pad_test = pad_sequences(encoded_test, maxlen=max_length, padding='pre')\n",
    "preds = np.argmax(model.predict(pad_test, verbose=0), axis=1)\n",
    "\n",
    "## Invert the dictionary to map values back to keys\n",
    "inverse_mapping = {v: k for k, v in target_mapping.items()}\n",
    "labels = [inverse_mapping[pred] for pred in preds]\n",
    "print(f\"Predictions : {labels}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
