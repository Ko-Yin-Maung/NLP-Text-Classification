{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Labels: [2 2 2 2 2 1 1 1 1 1 0 0 0 0 0]\n",
      "Classes: ['Anger' 'Love' 'Sadness']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset = [\n",
    "    \"သူမသည် ဖျက်သိမ်း စာကို ဖတ်နေစဉ် မျက်ရည်များ စီးကျလို့ နေသည်။\",\n",
    "    \"အခန်းသည် သူမ မရှိခြင်းကြောင့် တိတ်ဆိတ်မှုတို့ဖြင့် လွှမ်းမိုး စေခဲ့သည်။\",\n",
    "    \"သူသည် အလင်းမရှိသော နေရာတွင် တစ်ယောက်တည်း ထိုင်ပြီး ဝမ်းနည်းမှု၏ အငွေ့အသက်ကို ခံစားနေသည်။\",\n",
    "    \"သူမ၏ နှလုံးသားသည် မပြောလိုက်သော စကားများ၏ နာကျင်မှုကြောင့် ဝမ်းနည်းနေသည်။\",\n",
    "    \"မိုးသည် သူမ၏ စိတ်ခံစားချက်ကို စာနာပြီး ဝမ်းနည်းစွာ ကောင်းကင်မှ မျက်ရည်စက်များ အဖြစ် ရွာချပေးလို့ နေသည်။\",\n",
    "\n",
    "    \"သူ၏ အပြုံးက တစ်ခန်းလုံးကို ထွန်းလင်းစေပြီး သူမ၏ နှလုံးသားကို ပူနွေး စေခဲ့သည်။\",\n",
    "    \"သူမနှင့် တွေ့ဆုံတိုင်း ခံစားချက်တို့သည် အိမ်မက် တစ်ခု အလား အလွန်ပင် သာယာလှပလို့ နေသည်။\",\n",
    "    \"အချစ်၏ စွမ်းအားတို့သည် ချစ်သူ နှစ်ဦး ကြားရှိ အကွာအဝေးကို ဖျက်ဆီးပစ်နိုင်သည်။\",\n",
    "    \"အချစ်သည် သက်တမ်းမဲ့ တန်ခိုး တစ်ခု ဖြစ်ပြီး နှလုံးသားများကို နူးညံ့စေသည်။\",\n",
    "    \"အချစ်ကို သန့်ရှင်းစွာ ထိန်းသိမ်းနိုင်ပါက ဘဝ၏ အလှတရားကို ခံစားရမည်။\",\n",
    "\n",
    "    \"သူ၏ လက်သီးကို ကျစ်ကျစ်ဆုတ်ထားပြီး ရန်ငြိုးကို ထိန်းထားရန် ကြိုးစားနေသည်။\",\n",
    "    \"သူမသည် တံခါးကို ပိတ်ချပြီး ဆူညံစွာ အော်ဟစ်လို့ နေသည်။\",\n",
    "    \"သူမ၏ မျက်လုံးများတွင် ဒေါသ စိတ်တို့ဖြင့် မီးဟုန်းဟုန်း လောင်နေသည်။\",\n",
    "    \"အငြင်းပွားမှုသည် မာနကြောင့် တိုး၍ တိုး၍ လာသည်။\",\n",
    "    \"သူမ၏ အသံသည် စိတ်ဆိုးမှုကြောင့် တုန်လှုပ်နေပြီး စူးစိမ့် လှသည်။\"\n",
    "]\n",
    "targets = [\"Sadness\", \"Sadness\", \"Sadness\", \"Sadness\", \"Sadness\", \"Love\", \"Love\", \"Love\", \"Love\", \"Love\", \"Anger\", \"Anger\", \"Anger\", \"Anger\", \"Anger\"]\n",
    "\n",
    "## Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(targets)\n",
    "print(\"Encoded Labels:\", encoded_labels)\n",
    "print(\"Classes:\", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word length of sentence of dataset : 10\n",
      "Vocab Length : 92\n",
      "{'ကောင်းကင်မှ', 'စိတ်တို့ဖြင့်', 'စကားများ၏', 'ဝမ်းနည်းစွာ', 'ကျစ်ကျစ်ဆုတ်ထားပြီး', 'အကွာအဝေးကို', 'အငြင်းပွားမှုသည်', 'ထွန်းလင်းစေပြီး', 'တစ်ခန်းလုံးကို', 'မျက်ရည်များ', 'လက်သီးကို', 'အလွန်ပင်', 'အလှတရားကို', 'သူမ၏', 'ဘဝ၏', 'တိတ်ဆိတ်မှုတို့ဖြင့်', 'တန်ခိုး', 'အလင်းမရှိသော', 'ကြားရှိ', 'စာကို', 'ဒေါသ', 'စေခဲ့သည်။', 'စီးကျလို့', 'မိုးသည်', 'နှစ်ဦး', 'ရန်ငြိုးကို', 'နှလုံးသားများကို', 'နာကျင်မှုကြောင့်', 'တိုး၍', 'သာယာလှပလို့', 'အချစ်၏', 'ခံစားနေသည်။', 'မီးဟုန်းဟုန်း', 'ခံစားချက်တို့သည်', 'ဝမ်းနည်းနေသည်။', 'ဝမ်းနည်းမှု၏', 'တွေ့ဆုံတိုင်း', 'ခံစားရမည်။', 'ပူနွေး', 'စူးစိမ့်', 'အပြုံးက', 'စာနာပြီး', 'လောင်နေသည်။', 'စွမ်းအားတို့သည်', 'ဖတ်နေစဉ်', 'တစ်ခု', 'ဆူညံစွာ', 'ဖျက်သိမ်း', 'တံခါးကို', 'ထိုင်ပြီး', 'အငွေ့အသက်ကို', 'မာနကြောင့်', 'သန့်ရှင်းစွာ', 'တစ်ယောက်တည်း', 'အော်ဟစ်လို့', 'မျက်ရည်စက်များ', 'လှသည်။', 'သူမနှင့်', 'မျက်လုံးများတွင်', 'မရှိခြင်းကြောင့်', 'အချစ်သည်', 'သူ၏', 'နှလုံးသားကို', 'ပိတ်ချပြီး', '<OOV>', 'ထိန်းသိမ်းနိုင်ပါက', 'လာသည်။', 'ကြိုးစားနေသည်။', 'အလား', 'သူသည်', 'အချစ်ကို', 'နူးညံ့စေသည်။', 'မပြောလိုက်သော', 'ဖျက်ဆီးပစ်နိုင်သည်။', 'နေသည်။', 'ဖြစ်ပြီး', 'ရွာချပေးလို့', 'အဖြစ်', 'ထိန်းထားရန်', 'စိတ်ခံစားချက်ကို', 'အသံသည်', 'နေရာတွင်', 'ချစ်သူ', 'တုန်လှုပ်နေပြီး', 'အခန်းသည်', 'သက်တမ်းမဲ့', 'အိမ်မက်', 'လွှမ်းမိုး', 'နှလုံးသားသည်', 'သူမသည်', 'စိတ်ဆိုးမှုကြောင့်', 'သူမ'}\n"
     ]
    }
   ],
   "source": [
    "## Step 1: Tokenize the sentences to create a vocabulary, including OOV token\n",
    "def tokenize_sentences(dataset):\n",
    "    unique_words = set()\n",
    "    max_len = 0\n",
    "    for sentence in dataset:\n",
    "        words = sentence.lower().split()\n",
    "        max_len = max(len(words), max_len)\n",
    "        unique_words.update(words)\n",
    "    return unique_words, max_len\n",
    "\n",
    "# Create vocabulary and add <OOV> token\n",
    "vocabulary, max_len = tokenize_sentences(dataset)\n",
    "vocabulary.add('<OOV>')\n",
    "\n",
    "print(f'Max word length of sentence of dataset : {max_len}')\n",
    "print(f'Vocab Length : {len(vocabulary)}')\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 2: Convert each sentence into a one-hot-encoded matrix with padding\n",
    "def one_hot_encode(dataset, vocabulary, max_len):\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocabulary)}    \n",
    "    encoded_sentences = []    \n",
    "    for sentence in dataset:  \n",
    "        words = sentence.lower().split()      \n",
    "        sentence_encoding = np.zeros((max_len, len(vocabulary)))\n",
    "        for i, word in enumerate(words):\n",
    "            ## Use the OOV token if the word is not in the vocabulary\n",
    "            index = word2idx.get(word, word2idx['<OOV>'])\n",
    "            sentence_encoding[i, index] = 1        \n",
    "        encoded_sentences.append(sentence_encoding.flatten())    \n",
    "    return encoded_sentences\n",
    "\n",
    "encoded_dataset = one_hot_encode(dataset, vocabulary, max_len)\n",
    "encoded_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.16666666666666669\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_A = np.linalg.norm(A)\n",
    "    norm_B = np.linalg.norm(B)\n",
    "    return dot_product / (norm_A * norm_B)\n",
    "\n",
    "similarity = cosine_similarity(encoded_dataset[12], encoded_dataset[14])\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Anger\n"
     ]
    }
   ],
   "source": [
    "test = [\"သူမ၏ ဒေါသ အား ထိန်းထားရန် ခက်ခဲ လှသည်။\"]\n",
    "encoded_test = one_hot_encode(test, vocabulary, max_len)\n",
    "preds = [cosine_similarity(source, encoded_test[0]) for source in encoded_dataset]\n",
    "idx = encoded_labels[np.argmax(preds)]\n",
    "print(f'Predicted Label: {label_encoder.classes_[idx]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
